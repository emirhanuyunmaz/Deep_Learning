{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d2c446-e9ca-495c-ab68-0cf0441bf9bd",
   "metadata": {},
   "source": [
    "# Derin Pekiştirmeli Öğrenme (Deep Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85bfafe6-143f-4218-b96e-6b1c5d9cb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #reinforcement için env sağlar , geliştirme ortamı sağlar\n",
    "import numpy as np\n",
    "from collections import deque # Ajanın belleğini tanımlamak için deque veri yapısı\n",
    "from tensorflow.keras.models import Sequential #Sıralı model oluşturmak için \n",
    "from tensorflow.keras.layers import Dense  # Tam bağlı (Dense) katamanları\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm #İlerlemeyi görselleştirmek için "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47267449-b58c-47a7-97a1-f2e1717eba73",
   "metadata": {},
   "source": [
    "### 1)DQL Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef323da-b113-496c-b201-9da15687d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "\n",
    "    # Parametreleri ve hiper parametreleri tanımlama\n",
    "    def __init__(self,env):\n",
    "        \n",
    "        # Çevrenin gözlem alanı (state) boyut sayısı\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "\n",
    "        # Çevrede bulunan eylem sayısı\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "        # Gelecekteki odüllerin indirim oranı\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        #Learning rate : Ajanın öğrenme hızı\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        # Keşfetme oranı (epsilon) epsilon = 1 olsun max. keşif \n",
    "        self.epsilon = 1\n",
    "\n",
    "        # Epsilon her iterasyonda azalma oranı (epsilon azaldıkça daha fazla öğrenme , daha az keşif)\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        # Min. keşfetme oranı (epsilon : 0.01'in altına inemez)\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # Ajanın deneyimleri = Bellek = Geçmiş adımlar\n",
    "        self.memory = deque(maxlen = 1000)\n",
    "\n",
    "        # Derin öğrenme modelini inşaa et\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # DQL sinir ağı modeli oluşturma\n",
    "    def build_model(self):\n",
    "        model = Sequential() # Sıralı model\n",
    "\n",
    "        # Girdi katmanı , 48 nöron , relu aktivasyon\n",
    "        model.add(Dense(48,input_dim = self.state_size , activation = \"relu\" ))\n",
    "\n",
    "        # 24 Nöronlu 2. Gizli katman\n",
    "        model.add(Dense(24 , activation = \"relu\" ))\n",
    "\n",
    "        #Output Katmanı\n",
    "        model.add(Dense(self.action_size , activation = \"linear\" ))\n",
    "\n",
    "        # Modelin Derlenmesi\n",
    "        model.compile(loss = \"mse\" , optimizer = Adam(learning_rate = self.learning_rate ) )\n",
    "\n",
    "        return model\n",
    "    \n",
    "    # Ajanın deneyimlerini bellek veri yapısına kaydetme \n",
    "    def remember(self , state , action , reward , next_state , done ):\n",
    "        self.memory.append((state , action , reward , next_state , done)) \n",
    "\n",
    "    # Ajanın eylem seçebilme işlemi\n",
    "    def act(self , state):\n",
    "\n",
    "        #Eğer rastgele üretilen sayı epsilondan küçükse rastgele eylem seçilir\n",
    "        if random.uniform(0,1) <= self.epsilon :\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # Aksi durumda model tarafından tahmin edilen değere göre en iyi eylem seçilir \n",
    "        act_values = self.model.predict(state , verbose = 0)\n",
    "\n",
    "        # En yüksek değere sahip eylemi seç\n",
    "        return np.argmax( act_values[0] )\n",
    "\n",
    "    # Deneyimleri tekrar oynatarak deep q ağı eğitilir\n",
    "    def replay(self , batch_size):\n",
    "        \n",
    "        # Bellekte yeterinde deneyim yoksa geri oynatma yapılmaz\n",
    "        if len(self.memory) < batch_size :\n",
    "            return\n",
    "\n",
    "        # Bellekte rastgele batch size kadar deneyim seç\n",
    "        minibatch = random.sample(self.memory , batch_size )\n",
    "\n",
    "        for state , action , reward , next_state , done in minibatch :\n",
    "            \n",
    "            # Eğer done ise bitiş durumu var ise ödülü doğrudan hedef olarak alırız .\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state , verbose = 0)[0])\n",
    "\n",
    "            # Modelin tahmin ettiği ödüller \n",
    "            train_target = self.model.predict(state , verbose = 0)\n",
    "\n",
    "            # Ajanın yaptığı eyleme göre ödülü güncelle\n",
    "            train_target[0][action] = target\n",
    "\n",
    "            # Modeli eğit\n",
    "            self.model.fit(state , train_target , verbose = 0)\n",
    "    # Epsilonun zamanla azalmasını yani keşif ve sömürü dengesi\n",
    "    def adaptiveEGreedy(self):\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min :\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4ec99-29d4-4dd2-82a6-4aa84cd5b488",
   "metadata": {},
   "source": [
    "### 2)Model Eğitimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91447cb-8231-4f98-9d5b-9fc949f7c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emirh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\emirh\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:00<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodes : 2 Time : 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodes : 2 Time : 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\" , render_mode = \"human\") # Carpole ortamını başlatıyoruz\n",
    "agent = DQLAgent(env)\n",
    "\n",
    "batch_size = 32 # Eğitim için minibatch boyutu\n",
    "episodes = 2# Epochs , simülasyonun oynatılacağı bölüm sayısı\n",
    "\n",
    "for e in tqdm(range(episodes)):\n",
    "\n",
    "    # Ortamı sıfırla başlangıç durumunu al\n",
    "    state = env.reset()[0] # Ortamı sıfırlamak\n",
    "    state = np.reshape(state ,[1,4])\n",
    "\n",
    "    time = 0 # Zaman dilimini başlat\n",
    "\n",
    "    while True :\n",
    "        # Ajan Eylem seçer \n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Ajan ortamda bir eylem uygular ve bu eylem sonucunda next_state , reward , done (bitiş bilgisi) alır.\n",
    "        (next_state , reward , done , _ , _ ) = env.step(action)\n",
    "        next_state = np.reshape(state,[1,4])\n",
    "\n",
    "        # Yapmış olduğu bu adımı yani eylemi ve bu eylem sonucu env den alınan bilgileri kaydeder\n",
    "        agent.remember(state , action , reward , next_state , done)\n",
    "\n",
    "        #Mevcut durumu günceller\n",
    "        state = next_state \n",
    "\n",
    "        # Deneyimlerden yeniden oynatmayı başlatır\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "        # Epsilonu set eder \n",
    "        agent.adaptiveEGreedy()\n",
    "\n",
    "        # Zaman dilimini arttırır\n",
    "        time = time + 1\n",
    "\n",
    "        # Eğer done ise döngüyü kırar ve bölüm biter ve yeni bölüm başlar\n",
    "        if done :\n",
    "            print(f\"\\nEpisodes : {episodes} Time : {time}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa1e17-6120-457b-a9a4-f72741f61381",
   "metadata": {},
   "source": [
    "### 3)Modelin Test Edilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f30470f-a650-46ec-8ae0-4555c93204ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 1\n",
      "Time : 2\n",
      "Time : 3\n",
      "Time : 4\n",
      "Time : 5\n",
      "Time : 6\n",
      "Time : 7\n",
      "Time : 8\n",
      "Time : 9\n",
      "Time : 10\n",
      "Time : 11\n",
      "Time : 12\n",
      "Time : 13\n",
      "Time : 14\n",
      "Time : 15\n",
      "Time : 16\n",
      "Time : 17\n",
      "Time : 18\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "trained_model = agent # Eğitilen modeli al\n",
    "env = gym.make(\"CartPole-v1\",render_mode = \"human\") #Cartpole ortamını başlatıyoruz\n",
    "state = env.reset()[0]\n",
    "state = np.reshape(state , [1,4])\n",
    "\n",
    "time_t = 0 # Zaman adımını başlat\n",
    "\n",
    "while True:\n",
    "    # Ortamı Görsel olarak render et\n",
    "    env.render() \n",
    "\n",
    "    # Eğitilen modelde action gerçekleştir\n",
    "    action = trained_model.act(state)\n",
    "\n",
    "    #Eylemi Uygula\n",
    "    (next_state , reward , done , _ , _ ) = env.step(action)\n",
    "\n",
    "    next_state = np.reshape(next_state,[1,4])\n",
    "    state = next_state # Durumun güncellenmesi\n",
    "\n",
    "    time_t = time_t + 1\n",
    "    print(f\"Time : {time_t}\")\n",
    "\n",
    "    time.sleep(0.5) # 0.5 saniye bekle\n",
    "\n",
    "    if done :\n",
    "        break\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea46ee-7870-4b4b-b36a-b5d188a7d202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
