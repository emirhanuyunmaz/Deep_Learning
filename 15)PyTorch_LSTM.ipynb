{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7c7511-760f-4b9c-9dbb-e94287e37fce",
   "metadata": {},
   "source": [
    "# Uzun Kısa Süreli Bellek (Long Short Term Memory) (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929f9d10-6914-425c-a930-1e8458daa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter # Kelime frekanslarını hesaplama\n",
    "from itertools import product # Grid Search için kombinasyon oluşturma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d7cc2-814e-464e-8857-821f2e97e30e",
   "metadata": {},
   "source": [
    "**LSTM** ile metin üretme . Bellek sorunlarını çözmek için geliştirilmiş bir tür yinelemeli sinir ağı ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618a4d6e-41e2-45c6-9dd1-74f30197fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ürün Yorumları\n",
    "text = \"\"\"Bu ürün beklentimi fazlasıyla karşıladı.  \n",
    "Malzeme kalitesi gerçekten çok iyi.  \n",
    "Kargo hızlı ve sorunsuz bir şekilde elime ulaştı.  \n",
    "Fiyatına göre performansı harika.  \n",
    "Kesinlikle tavsiye ederim ve öneririm!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff872bf7-dd9f-45b6-9bdb-ac930df5cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri Ön İşleme\n",
    "# Noktalama İşaretlerinden Kurtul\n",
    "# Küçük Harf Dönüşümü\n",
    "# Kelimeleri böl\n",
    "words = text.replace(\".\",\"\").replace(\"!\",\"\").lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aac70f6-6024-4978-a621-cb91880095ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelime frekanslarını hesaplama \n",
    "word_counts = Counter(words)\n",
    "vocab = sorted(word_counts , key = word_counts.get , reverse = True) # Kelimelerin frekansı\n",
    "# Büyükten küçüğe sıralama \n",
    "word_to_ix = {word : i for i , word in enumerate(vocab)}\n",
    "ix_to_word = {i : word for i , word in enumerate(vocab)}\n",
    "\n",
    "# Eğitim verisi hazırlama \n",
    "data = [(words[i] , words[i + 1] ) for i in range(len(words) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2a6f59-f7c8-4afa-a8c3-07021918be12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bu', 'ürün'),\n",
       " ('ürün', 'beklentimi'),\n",
       " ('beklentimi', 'fazlasıyla'),\n",
       " ('fazlasıyla', 'karşıladı'),\n",
       " ('karşıladı', 'malzeme'),\n",
       " ('malzeme', 'kalitesi'),\n",
       " ('kalitesi', 'gerçekten'),\n",
       " ('gerçekten', 'çok'),\n",
       " ('çok', 'iyi'),\n",
       " ('iyi', 'kargo'),\n",
       " ('kargo', 'hızlı'),\n",
       " ('hızlı', 've'),\n",
       " ('ve', 'sorunsuz'),\n",
       " ('sorunsuz', 'bir'),\n",
       " ('bir', 'şekilde'),\n",
       " ('şekilde', 'elime'),\n",
       " ('elime', 'ulaştı'),\n",
       " ('ulaştı', 'fiyatına'),\n",
       " ('fiyatına', 'göre'),\n",
       " ('göre', 'performansı'),\n",
       " ('performansı', 'harika'),\n",
       " ('harika', 'kesinlikle'),\n",
       " ('kesinlikle', 'tavsiye'),\n",
       " ('tavsiye', 'ederim'),\n",
       " ('ederim', 've'),\n",
       " ('ve', 'öneririm')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa26c21-1b60-4c71-b7a1-88f0c26e5c30",
   "metadata": {},
   "source": [
    "**LSTM Modelin Oluşturulması**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb98043-8356-4d6f-a6a5-bfa8eae33cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self , vocab_size , embedding_dim , hidden_dim):\n",
    "        super(LSTM,self).__init__() # Bir üst sınıfın constructor ini çağırma \n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim , hidden_dim) # LSTM Katmanı\n",
    "        self.fc = nn.Linear(hidden_dim , vocab_size)\n",
    "\n",
    "    def forward(self , x):\n",
    "        \"\"\"\n",
    "            input -> embedding -> lstm -> fc -> output\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.embedding(x) \n",
    "        lstm_out , _ = self.lstm(x.view(1,1,-1))\n",
    "        output = self.fc(lstm_out.view(1,-1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2d9b87-d2c7-464a-a8d3-f89a79a8ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(len(vocab) , embedding_dim = 8 , hidden_dim = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c20e8-7d39-4671-a100-ef54fef64da7",
   "metadata": {},
   "source": [
    "**Hiper Parametrelerin Belirlenmesi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4eeae83-138c-4526-bfe6-6270af78dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelime Listesi -> Tensor\n",
    "def prepare_sequence(seq , to_ix):\n",
    "    return torch.tensor([to_ix[w] for w in seq] , dtype = torch.long )\n",
    "\n",
    "# Hyperparameter tuning kombinasyonunun belirlenmesi\n",
    "embedding_size = [8 , 16] # Denenecek embedding boyutları\n",
    "hidden_sizes = [32 , 64] # Denenecek gizli kataman boyutları\n",
    "learning_rates = [0.01 , 0.005] # Öğrenme oranı\n",
    "\n",
    "best_loss = float(\"inf\") # En düşük kayıp değerini saklamak için bir değişken\n",
    "best_params = {} # En iyi parametreleri saklamak için boş bir dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b043ccd-bd14-4a1e-b099-96164a63bb33",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be74483c-4eb3-4a6c-803f-c5f662a99b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding :8  -  Hidden Size : 32 - Learning Rate : 0.01 \n",
      "Epochs : 0 , Loss : 86.89599\n",
      "Epochs : 10 , Loss : 3.85817\n",
      "Epochs : 20 , Loss : 2.29824\n",
      "Epochs : 30 , Loss : 2.01193\n",
      "Epochs : 40 , Loss : 1.89713\n",
      "\n",
      "Embedding :8  -  Hidden Size : 32 - Learning Rate : 0.005 \n",
      "Epochs : 0 , Loss : 85.34375\n",
      "Epochs : 10 , Loss : 14.47495\n",
      "Epochs : 20 , Loss : 3.61015\n",
      "Epochs : 30 , Loss : 2.45563\n",
      "Epochs : 40 , Loss : 2.08991\n",
      "\n",
      "Embedding :8  -  Hidden Size : 64 - Learning Rate : 0.01 \n",
      "Epochs : 0 , Loss : 86.60152\n",
      "Epochs : 10 , Loss : 2.63809\n",
      "Epochs : 20 , Loss : 2.07843\n",
      "Epochs : 30 , Loss : 1.93907\n",
      "Epochs : 40 , Loss : 1.85579\n",
      "\n",
      "Embedding :8  -  Hidden Size : 64 - Learning Rate : 0.005 \n",
      "Epochs : 0 , Loss : 85.76938\n",
      "Epochs : 10 , Loss : 4.82903\n",
      "Epochs : 20 , Loss : 2.33944\n",
      "Epochs : 30 , Loss : 2.00226\n",
      "Epochs : 40 , Loss : 1.86772\n",
      "\n",
      "Embedding :16  -  Hidden Size : 32 - Learning Rate : 0.01 \n",
      "Epochs : 0 , Loss : 86.42317\n",
      "Epochs : 10 , Loss : 3.03618\n",
      "Epochs : 20 , Loss : 2.14498\n",
      "Epochs : 30 , Loss : 1.95486\n",
      "Epochs : 40 , Loss : 1.86510\n",
      "\n",
      "Embedding :16  -  Hidden Size : 32 - Learning Rate : 0.005 \n",
      "Epochs : 0 , Loss : 85.71434\n",
      "Epochs : 10 , Loss : 8.79019\n",
      "Epochs : 20 , Loss : 2.90854\n",
      "Epochs : 30 , Loss : 2.20781\n",
      "Epochs : 40 , Loss : 1.96802\n",
      "\n",
      "Embedding :16  -  Hidden Size : 64 - Learning Rate : 0.01 \n",
      "Epochs : 0 , Loss : 86.86908\n",
      "Epochs : 10 , Loss : 2.50277\n",
      "Epochs : 20 , Loss : 2.02295\n",
      "Epochs : 30 , Loss : 1.87762\n",
      "Epochs : 40 , Loss : 1.80283\n",
      "\n",
      "Embedding :16  -  Hidden Size : 64 - Learning Rate : 0.005 \n",
      "Epochs : 0 , Loss : 85.40442\n",
      "Epochs : 10 , Loss : 3.50603\n",
      "Epochs : 20 , Loss : 2.11286\n",
      "Epochs : 30 , Loss : 1.87307\n",
      "Epochs : 40 , Loss : 1.77524\n",
      "\n",
      "Best Params : {'embedding_dim': 16, 'hidden_dim': 64, 'learning_rate': 0.005}\n"
     ]
    }
   ],
   "source": [
    "for emb_size , hidden_size , lr in product(embedding_size , hidden_sizes , learning_rates):\n",
    "    print(f\"Embedding :{emb_size}  -  Hidden Size : {hidden_size} - Learning Rate : {lr} \")\n",
    "\n",
    "    # Modeli Tanımla\n",
    "    model = LSTM(len(vocab) , emb_size , hidden_size) # Seçilen parametreler ile model oluşturma\n",
    "    loss_function = nn.CrossEntropyLoss() # Entropi kayıp fonksiyonu\n",
    "    optimizer = optim.Adam(model.parameters() , lr = lr) # Seçilen lr ile Adam optimizeri\n",
    "\n",
    "    epochs = 50\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0 \n",
    "        \n",
    "        for word , next_word in data :\n",
    "        \n",
    "            model.zero_grad() # Gradyanları Sıfırla\n",
    "            input_tensor = prepare_sequence([word] , word_to_ix) # Girdiyi tensore çevir\n",
    "            target_tensor = prepare_sequence([next_word] , word_to_ix) # Hedef kelimeyi tensore çevirme\n",
    "            output = model(input_tensor ) # Prediction\n",
    "            loss = loss_function(output , target_tensor)\n",
    "            loss.backward() # Geri yayılım işlemi uygulama\n",
    "            optimizer.step() # Parametreleri güncelle\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epochs : {epoch} , Loss : {epoch_loss:.5f}\")\n",
    "        total_loss = epoch_loss\n",
    "\n",
    "    # En İyi Modeli Kaydedelim\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_params = {\"embedding_dim\":emb_size , \"hidden_dim\":hidden_size , \"learning_rate\":lr}\n",
    "    print()\n",
    "print(f\"Best Params : {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c927630-7a47-4c23-a78d-2cf140cfde40",
   "metadata": {},
   "source": [
    "**En iyi parametreler ile eğitim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7fbb66f-38ec-440b-ab97-1d0cbf0132a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Epoch : 0 , Loss : 85.52355\n",
      "Final Model Epoch : 10 , Loss : 3.26544\n",
      "Final Model Epoch : 20 , Loss : 2.11454\n",
      "Final Model Epoch : 30 , Loss : 1.89104\n",
      "Final Model Epoch : 40 , Loss : 1.79326\n",
      "Final Model Epoch : 50 , Loss : 1.73621\n",
      "Final Model Epoch : 60 , Loss : 1.69809\n",
      "Final Model Epoch : 70 , Loss : 1.67261\n",
      "Final Model Epoch : 80 , Loss : 1.65028\n",
      "Final Model Epoch : 90 , Loss : 1.63324\n"
     ]
    }
   ],
   "source": [
    "final_model = LSTM(len(vocab),best_params[\"embedding_dim\"] , best_params[\"hidden_dim\"])\n",
    "optimizer = optim.Adam(final_model.parameters(),lr = best_params[\"learning_rate\"])\n",
    "loss_function = nn.CrossEntropyLoss() # Entropi kayıp fonk.\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for word , next_word in data:\n",
    "        final_model.zero_grad()\n",
    "        input_tensor = prepare_sequence([word],word_to_ix)\n",
    "        target_tensor = prepare_sequence([next_word],word_to_ix)\n",
    "        output = final_model(input_tensor)\n",
    "        loss = loss_function(output , target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if epoch % 10 == 0 :\n",
    "        print(f\"Final Model Epoch : {epoch} , Loss : {epoch_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e073e2d-e057-4a20-a5e8-0ffb687637f1",
   "metadata": {},
   "source": [
    "**Modelin Test Edilmesi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42db8a31-453b-4529-be48-b37dca76d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelime tahmin fonc\n",
    "def predict_sequence(start_word , num_words):\n",
    "    current_word = start_word # Şu anki kelime başlangıç kelimesi olarak ayarlanır\n",
    "    output_sequence = [current_word] # Çıktı dizisi \n",
    "\n",
    "    for _ in range(num_words): # Belirlenen sayıda kelime tahmini\n",
    "        with torch.no_grad():\n",
    "            input_tensor = prepare_sequence([current_word] , word_to_ix) # kelime -> tensor\n",
    "            output = final_model(input_tensor)\n",
    "            predicted_ix = torch.argmax(output).item() # En yüksek olasılığa ait kelime indexi\n",
    "            predicted_word = ix_to_word[predicted_ix] # İndexe karşılık gelen kelimeyi return et\n",
    "            output_sequence.append(predicted_word)\n",
    "            current_word = predicted_word # Bir sonraki tahmin için mevcut kelimeleri güncelle  \n",
    "    return output_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bffdbc54-b7a5-4c66-9157-868b7fb59f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ürün beklentimi fazlasıyla karşıladı malzeme\n"
     ]
    }
   ],
   "source": [
    "start_word = \"ürün\"\n",
    "num_predictions = 4\n",
    "predicted_sequence = predict_sequence(start_word , num_predictions)\n",
    "print(\" \".join(predicted_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10404c76-ab5f-4da5-afe7-7a6fdf42a6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
